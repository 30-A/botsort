{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4141dfc9-a797-4521-977a-1d305e8ac670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from moviepy.editor import VideoFileClip\n",
    "import imageio\n",
    "import imageio_ffmpeg as ffmpeg\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction, get_prediction\n",
    "\n",
    "from botsort.bot_sort import BoTSORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4cb98-0202-4cca-91e6-7097487d7079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Video:\n",
    "    def __init__(self, video_path):\n",
    "        self.video_path = video_path\n",
    "        self.clip = VideoFileClip(video_path)\n",
    "        self.fps = self.clip.fps\n",
    "        self.total_frames = int(self.clip.fps * self.clip.duration)\n",
    "        self.size = (int(self.clip.size[0]), int(self.clip.size[1]))\n",
    "\n",
    "    def get_video_info(self):\n",
    "        return {\n",
    "            'fps': self.fps,\n",
    "            'total_frames': self.total_frames,\n",
    "            'duration': round(self.clip.duration, 2),\n",
    "            'size': self.size\n",
    "        }\n",
    "\n",
    "    def _process_frames(self, new_fps, new_size, output_path):\n",
    "        def process_frame(frame):\n",
    "            if new_size:\n",
    "                frame = cv2.resize(frame, new_size, interpolation=cv2.INTER_AREA)\n",
    "            return frame\n",
    "\n",
    "        if new_fps:\n",
    "            self.clip = self.clip.set_fps(new_fps)\n",
    "        if new_size:\n",
    "            self.clip = self.clip.fl_image(process_frame)\n",
    "\n",
    "        self.clip.write_videofile(output_path, fps=new_fps if new_fps else self.fps)\n",
    "        return self.total_frames\n",
    "\n",
    "    def reduce_fps(self, new_fps, output_path):\n",
    "        if new_fps >= self.fps:\n",
    "            raise ValueError(\"New FPS must be lower than the current FPS.\")\n",
    "\n",
    "        new_total_frames = self._process_frames(new_fps, None, output_path)\n",
    "        new_duration = round(self.clip.duration, 2)\n",
    "\n",
    "        return {\n",
    "            'fps': new_fps,\n",
    "            'total_frames': new_total_frames,\n",
    "            'duration': new_duration,\n",
    "            'size': self.size\n",
    "        }\n",
    "\n",
    "    def reduce_size(self, scale_factor, output_path):\n",
    "        if scale_factor >= 1:\n",
    "            raise ValueError(\"Scale factor must be less than 1.\")\n",
    "\n",
    "        new_size = (\n",
    "            int(self.size[0] * scale_factor),\n",
    "            int(self.size[1] * scale_factor)\n",
    "        )\n",
    "        new_total_frames = self._process_frames(self.fps, new_size, output_path)\n",
    "        new_duration = round(self.clip.duration, 2)\n",
    "\n",
    "        return {\n",
    "            'fps': self.fps,\n",
    "            'total_frames': new_total_frames,\n",
    "            'duration': new_duration,\n",
    "            'size': new_size\n",
    "        }\n",
    "\n",
    "    def reduce_fps_and_size(self, new_fps, scale_factor, output_path):\n",
    "        if new_fps >= self.fps or scale_factor >= 1:\n",
    "            raise ValueError(\"New FPS must be lower and scale factor must be less than 1.\")\n",
    "\n",
    "        new_size = (\n",
    "            int(self.size[0] * scale_factor),\n",
    "            int(self.size[1] * scale_factor)\n",
    "        )\n",
    "        new_total_frames = self._process_frames(new_fps, new_size, output_path)\n",
    "        new_duration = round(self.clip.duration, 2)\n",
    "\n",
    "        return {\n",
    "            'fps': new_fps,\n",
    "            'total_frames': new_total_frames,\n",
    "            'duration': new_duration,\n",
    "            'size': new_size\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd004e-4951-4181-9a2f-cb14f805c9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Detector:\n",
    "    def __init__(self, config_path, checkpoint_path, use_slicing=True, confidence_threshold=0.4):\n",
    "        self.config_path = config_path\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.use_slicing = use_slicing\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        if config_path and checkpoint_path:\n",
    "            self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        self.model = AutoDetectionModel.from_pretrained(\n",
    "            model_type='mmdet',\n",
    "            model_path=self.checkpoint_path,\n",
    "            config_path=self.config_path,\n",
    "            confidence_threshold=self.confidence_threshold,\n",
    "            category_mapping=None,\n",
    "            image_size=None,\n",
    "            device='cuda:0'\n",
    "        )\n",
    "        self.class_labels = self.model.model.model.dataset_meta.get(\"classes\")\n",
    "        self.class_colors = self.model.model.model.dataset_meta.get(\"palette\")\n",
    "        self.class_colors = {i: tuple(color) for i, color in enumerate(self.class_colors)}\n",
    "        self.class_labels = {i: label for i, label in enumerate(self.class_labels)}\n",
    "\n",
    "    def detect(self, bgr_frame, slice_height=720, slice_width=720, overlap_height_ratio=0.1, overlap_width_ratio=0.1):\n",
    "        # Convert BGR frame to RGB\n",
    "        rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.use_slicing:\n",
    "            result = get_sliced_prediction(\n",
    "                rgb_frame,\n",
    "                self.model,\n",
    "                slice_height=slice_height,\n",
    "                slice_width=slice_width,\n",
    "                postprocess_match_threshold=0.7,\n",
    "                postprocess_type=\"NMM\",\n",
    "                postprocess_class_agnostic=True,\n",
    "                overlap_height_ratio=overlap_height_ratio,\n",
    "                overlap_width_ratio=overlap_width_ratio,\n",
    "                verbose=0\n",
    "            )\n",
    "        else:\n",
    "            result = get_prediction(rgb_frame, self.model)\n",
    "        \n",
    "        object_prediction_list = result.object_prediction_list\n",
    "        detections = []\n",
    "        for obj in object_prediction_list:\n",
    "            bbox = obj.bbox.to_voc_bbox()\n",
    "            score = obj.score.value\n",
    "            label = obj.category.id\n",
    "            detections.append((np.array([bbox[0], bbox[1], bbox[2], bbox[3]]), score, label))\n",
    "        return detections\n",
    "\n",
    "    def get_class_labels(self):\n",
    "        return self.class_labels\n",
    "\n",
    "    def get_class_colors(self):\n",
    "        return self.class_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf99dcb-acff-4ca2-bc02-6415cbe6dd37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "    def __init__(self,\n",
    "                 track_high_thresh=0.6,\n",
    "                 track_low_thresh=0.1,\n",
    "                 new_track_thresh=0.7, \n",
    "                 track_buffer=15,\n",
    "                 proximity_thresh=0.4,\n",
    "                 cmc_method='sparseOptFlow',\n",
    "                 match_thresh=0.7):\n",
    "        self.track_high_thresh = track_high_thresh\n",
    "        self.track_low_thresh = track_low_thresh\n",
    "        self.new_track_thresh = new_track_thresh\n",
    "        self.track_buffer = track_buffer\n",
    "        self.proximity_thresh = proximity_thresh\n",
    "        self.cmc_method = cmc_method\n",
    "        self.match_thresh = match_thresh\n",
    "        self.tracker = BoTSORT(self)\n",
    "\n",
    "    def update(self, detections, frame):\n",
    "        return self.tracker.update(detections, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508b110-9735-44cf-bcf4-8d652529dbd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Counter:\n",
    "    def __init__(self, class_labels, class_colors, line_offset=0, draw_line=True):\n",
    "        self.class_labels = class_labels\n",
    "        self.class_colors = class_colors\n",
    "        self.line_offset = line_offset\n",
    "        self.crossed_line_ids = {cls_id: set() for cls_id in class_labels}\n",
    "        self.track_class_mapping = {}\n",
    "        self.track_trails = {}\n",
    "        self.draw_line = draw_line\n",
    "\n",
    "    def count_objects(self, tracked_objects, frame_width, frame):\n",
    "        vertical_line_x = (frame_width // 2) + self.line_offset\n",
    "\n",
    "        for t in tracked_objects:\n",
    "            track_id = int(t.track_id)\n",
    "            cls = t.cls\n",
    "\n",
    "            if track_id not in self.track_class_mapping:\n",
    "                self.track_class_mapping[track_id] = cls\n",
    "                self.track_trails[track_id] = []\n",
    "\n",
    "            bbox = t.tlwh\n",
    "            center = (int(bbox[0] + bbox[2] // 2), int(bbox[1] + bbox[3] // 2))\n",
    "            self.track_trails[track_id].append(center)\n",
    "\n",
    "            if len(self.track_trails[track_id]) > 1:\n",
    "                prev_center = self.track_trails[track_id][-2]\n",
    "                if (prev_center[0] < vertical_line_x and center[0] >= vertical_line_x) or (prev_center[0] > vertical_line_x and center[0] <= vertical_line_x):\n",
    "                    self.crossed_line_ids[cls].add(track_id)\n",
    "\n",
    "        if self.draw_line:\n",
    "            cv2.line(frame, (vertical_line_x, 0), (vertical_line_x, frame.shape[0]), (0, 255, 0), 2)\n",
    "\n",
    "        return self.track_trails, self.track_class_mapping\n",
    "\n",
    "    def get_counts(self):\n",
    "        return {self.class_labels[cls]: len(self.crossed_line_ids[cls]) for cls in self.class_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ee915-cd70-48e7-80b0-68a48542a873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Plotting:\n",
    "    def __init__(self, logo_path):\n",
    "        self.logo = cv2.imread(logo_path, cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "    def add_logo(self, frame, logo_padding=25):\n",
    "        logo_width = 235\n",
    "        scale = logo_width / self.logo.shape[1]\n",
    "        logo_resized = cv2.resize(self.logo, (logo_width, int(self.logo.shape[0] * scale)), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Change logo color to white while keeping transparency\n",
    "        logo_rgb = logo_resized[..., :3]\n",
    "        alpha_mask = logo_resized[..., 3] / 255.0\n",
    "\n",
    "        white_logo = np.ones_like(logo_rgb) * 255\n",
    "        logo_white = cv2.addWeighted(logo_rgb, 0, white_logo, 1, 0)\n",
    "\n",
    "        h, w = logo_resized.shape[:2]\n",
    "        x = frame.shape[1] - w - logo_padding\n",
    "        y = frame.shape[0] - h - logo_padding\n",
    "        \n",
    "        roi = frame[y:y+h, x:x+w].copy()  # Create a writable copy of the ROI\n",
    "        for c in range(3):\n",
    "            roi[:, :, c] = roi[:, :, c] * (1 - alpha_mask) + logo_white[:, :, c] * alpha_mask\n",
    "        frame[y:y+h, x:x+w] = roi\n",
    "        return frame\n",
    "\n",
    "    def add_class_count(self, frame, text_lines, alpha=0.3, padding=15, bottom_space=15, side_space=15):\n",
    "        x_offset = padding + side_space\n",
    "        y_offset = padding + bottom_space\n",
    "        max_text_width = max([cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0][0] for line in text_lines])\n",
    "        text_height = cv2.getTextSize(text_lines[0], cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0][1]\n",
    "        total_text_height = len(text_lines) * text_height + (len(text_lines) - 1) * 10  # 10 is the line spacing\n",
    "        overlay = frame.copy()\n",
    "\n",
    "        rect_x1 = x_offset - padding\n",
    "        rect_y1 = y_offset - padding\n",
    "        rect_x2 = x_offset + max_text_width + 50 + padding\n",
    "        rect_y2 = y_offset + total_text_height + 2 * padding\n",
    "        radius = 15\n",
    "\n",
    "        # Create a larger rectangle and subtract radius for circles\n",
    "        cv2.rectangle(overlay, (rect_x1 + radius, rect_y1), (rect_x2 - radius, rect_y2), (255, 255, 255), -1)\n",
    "        cv2.rectangle(overlay, (rect_x1, rect_y1 + radius), (rect_x2, rect_y2 - radius), (255, 255, 255), -1)\n",
    "        \n",
    "        # Draw the rounded corners\n",
    "        cv2.circle(overlay, (rect_x1 + radius, rect_y1 + radius), radius, (255, 255, 255), -1)\n",
    "        cv2.circle(overlay, (rect_x2 - radius, rect_y1 + radius), radius, (255, 255, 255), -1)\n",
    "        cv2.circle(overlay, (rect_x1 + radius, rect_y2 - radius), radius, (255, 255, 255), -1)\n",
    "        cv2.circle(overlay, (rect_x2 - radius, rect_y2 - radius), radius, (255, 255, 255), -1)\n",
    "        \n",
    "        cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "        \n",
    "        # Adjust the current_y to start from the correct offset to maintain padding consistency\n",
    "        current_y = y_offset + padding + text_height\n",
    "        for line in text_lines:\n",
    "            cv2.putText(frame, line, (x_offset + 30, current_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "            current_y += text_height + 10  # 10 is the line spacing\n",
    "        return frame\n",
    "\n",
    "    def add_circles(self, frame, text_lines, class_labels, class_colors, padding=15, bottom_space=15, side_space=15):\n",
    "        x_offset = padding + side_space\n",
    "        y_offset = padding + bottom_space + 22  # Adjusted to better align with text baseline\n",
    "        for line in text_lines:\n",
    "            class_name = line.split(':')[0].strip()\n",
    "            color = class_colors[list(class_labels.values()).index(class_name)]\n",
    "            circle_radius = 8\n",
    "            # Calculate text size to adjust the circle position\n",
    "            text_size = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]\n",
    "            text_height = text_size[1]\n",
    "            circle_y_offset = text_height // 2 - circle_radius // 2  # Adjusted to better align with text baseline\n",
    "            cv2.circle(frame, (x_offset + 15, y_offset + circle_y_offset), circle_radius, color, -1)\n",
    "            y_offset += text_height + 10  # 10 is the line spacing\n",
    "        return frame\n",
    "\n",
    "    def add_bbox_and_track_id(self, frame, bbox, track_id, color):\n",
    "        x1, y1, w, h = map(int, bbox)\n",
    "        x2, y2 = x1 + w, y1 + h\n",
    "        label = f'{track_id}'\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)\n",
    "        cv2.rectangle(frame, (x1, y1 - text_height - baseline), (x1 + text_width, y1), color, -1)\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "\n",
    "    def add_trails(self, frame, track_trails, track_id, color):\n",
    "        for point in track_trails[track_id]:\n",
    "            cv2.circle(frame, point, 2, color, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7395b16-0990-4f7a-a0c3-4166bd446323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Processor:\n",
    "    def __init__(self):\n",
    "        current_dir = os.getcwd()\n",
    "        self.output_dir = os.path.join(current_dir, 'output')\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self.logo_path = os.path.join(current_dir, 'logo.png')\n",
    "        self.detector = None\n",
    "        self.modified_video_path = None\n",
    "\n",
    "    def print_video_info(self, info, title):\n",
    "        print(f\"{title} Video:\")\n",
    "        print(f\"fps: {info['fps']}\")\n",
    "        print(f\"total frames: {info['total_frames']}\")\n",
    "        print(f\"duration: {info['duration']}\")\n",
    "        print(f\"size: {info['size']}\")\n",
    "        print()\n",
    "\n",
    "    def modify_video(self, video, reduce_fps=None, reduce_size=None):\n",
    "        modified = False\n",
    "        if reduce_fps and reduce_size:\n",
    "            output_path = os.path.join(self.output_dir, 'reduced_fps_and_size.mp4')\n",
    "            new_info = video.reduce_fps_and_size(reduce_fps, reduce_size, output_path)\n",
    "            modified = True\n",
    "        elif reduce_fps:\n",
    "            output_path = os.path.join(self.output_dir, 'reduced_fps.mp4')\n",
    "            new_info = video.reduce_fps(reduce_fps, output_path)\n",
    "            modified = True\n",
    "        elif reduce_size:\n",
    "            output_path = os.path.join(self.output_dir, 'reduced_size.mp4')\n",
    "            new_info = video.reduce_size(reduce_size, output_path)\n",
    "            modified = True\n",
    "        else:\n",
    "            output_path = video.video_path\n",
    "            new_info = video.get_video_info()\n",
    "        \n",
    "        self.modified_video_path = output_path\n",
    "        return new_info, modified\n",
    "\n",
    "    def run_detector(self, video_path, config_path, checkpoint_path, use_slicing=True, confidence_threshold=0.4, reduce_fps=None, reduce_size=None):\n",
    "        # Initialize Video class\n",
    "        video = Video(video_path)\n",
    "        info = video.get_video_info()\n",
    "        self.print_video_info(info, \"Original\")\n",
    "        \n",
    "        # Modify the video if needed\n",
    "        new_info, modified = self.modify_video(video, reduce_fps, reduce_size)\n",
    "        if modified:\n",
    "            self.print_video_info(new_info, \"Modified\")\n",
    "        \n",
    "        csv_output_path = os.path.join(self.output_dir, 'detections.csv')\n",
    "        self.detector = Detector(config_path, checkpoint_path, use_slicing, confidence_threshold)\n",
    "        reader = VideoFileClip(self.modified_video_path)\n",
    "        frame_width = int(reader.size[0])\n",
    "        total_frames = new_info['total_frames']\n",
    "        \n",
    "        frame_id = 1\n",
    "        \n",
    "        with open(csv_output_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"frame_id\", \"x1\", \"y1\", \"x2\", \"y2\", \"score\", \"label\"])  # Write the header\n",
    "            \n",
    "            with tqdm(total=total_frames, desc=\"Detecting\", unit=\"frame\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} frames [{elapsed}<{remaining}, {rate_fmt}]\") as pbar:\n",
    "                for frame in reader.iter_frames():\n",
    "                    bgr_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                    detections = self.detector.detect(bgr_frame)\n",
    "                    for bbox, score, label in detections:\n",
    "                        writer.writerow([frame_id, *bbox, score, label])  # Write each detection result immediately\n",
    "                    pbar.update(1)\n",
    "                    frame_id += 1\n",
    "        \n",
    "        print(f\"Detections saved to {csv_output_path}\")\n",
    "\n",
    "    def run_tracker(self,\n",
    "                    line_offset=0,\n",
    "                    draw_line=True,\n",
    "                    track_high_thresh=0.6,\n",
    "                    track_low_thresh=0.1,\n",
    "                    new_track_thresh=0.7,\n",
    "                    track_buffer=15,\n",
    "                    proximity_thresh=0.4,\n",
    "                    cmc_method='sparseOptFlow',\n",
    "                    match_thresh=0.7,\n",
    "                    use_tracker_only=False,\n",
    "                    detections_path=None, video_path=None, config_path=None, checkpoint_path=None):\n",
    "        if use_tracker_only:\n",
    "            if not detections_path or not video_path or not config_path or not checkpoint_path:\n",
    "                raise ValueError(\"When use_tracker_only is True, detections_path, video_path, config_path, and checkpoint_path must be provided.\")\n",
    "            \n",
    "            # Initialize the detector to access class labels and colors\n",
    "            self.detector = Detector(config_path, checkpoint_path, use_slicing=False, confidence_threshold=0.0)\n",
    "            self.modified_video_path = video_path\n",
    "            csv_input_path = detections_path\n",
    "        else:\n",
    "            if not self.modified_video_path:\n",
    "                raise ValueError(\"Run the detector first to set the modified video path.\")\n",
    "            csv_input_path = os.path.join(self.output_dir, 'detections.csv')\n",
    "\n",
    "        video_output_path = os.path.join(self.output_dir, 'counting_video.mp4')\n",
    "        tracker_csv_output_path = os.path.join(self.output_dir, 'tracker_output.csv')\n",
    "        reader = VideoFileClip(self.modified_video_path)\n",
    "        frame_width = int(reader.size[0])\n",
    "        fps = reader.fps\n",
    "        total_frames = Video(self.modified_video_path).get_video_info()['total_frames']\n",
    "\n",
    "        detections = pd.read_csv(csv_input_path)\n",
    "        frame_groups = detections.groupby('frame_id')\n",
    "\n",
    "        tracker = Tracker(track_high_thresh, track_low_thresh, new_track_thresh, track_buffer, proximity_thresh, cmc_method, match_thresh)\n",
    "        counter = Counter(self.detector.get_class_labels(), self.detector.get_class_colors(), line_offset, draw_line)\n",
    "        plotting = Plotting(self.logo_path)\n",
    "\n",
    "        # Open video writer\n",
    "        writer = imageio.get_writer(video_output_path, fps=fps)\n",
    "\n",
    "        # List to hold tracking data for CSV\n",
    "        tracking_data = []\n",
    "\n",
    "        with tqdm(total=total_frames, desc=\"Tracking\", unit=\"frame\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} frames [{elapsed}<{remaining}, {rate_fmt}]\") as pbar:\n",
    "            for frame_id, frame in enumerate(reader.iter_frames(), start=1):\n",
    "                frame = frame.copy()  # Create a writable copy of the frame\n",
    "                if frame_id in frame_groups.groups:\n",
    "                    frame_detections = frame_groups.get_group(frame_id)\n",
    "                    detection_list = frame_detections[['x1', 'y1', 'x2', 'y2', 'score', 'label']].values.tolist()\n",
    "                    detection_array = np.array(detection_list)\n",
    "                else:\n",
    "                    detection_array = np.empty((0, 6))\n",
    "\n",
    "                tracked_objects = tracker.update(detection_array, frame)\n",
    "                track_trails, track_class_mapping = counter.count_objects(tracked_objects, frame_width, frame)\n",
    "\n",
    "                for t in tracked_objects:\n",
    "                    bbox = t.tlwh\n",
    "                    track_id = int(t.track_id)\n",
    "                    cls = track_class_mapping[track_id]\n",
    "                    color = counter.class_colors[cls]\n",
    "                    plotting.add_bbox_and_track_id(frame, bbox, track_id, color)\n",
    "                    plotting.add_trails(frame, track_trails, track_id, color)\n",
    "\n",
    "                    # Add tracking data to list\n",
    "                    x, y, w, h = bbox\n",
    "                    tracking_data.append([frame_id, track_id, x, y, w, h, cls])\n",
    "\n",
    "                counts = counter.get_counts()\n",
    "                text_lines = [f\"{cls}: {count}\" for cls, count in counts.items()]\n",
    "                frame = plotting.add_class_count(frame, text_lines)\n",
    "                frame = plotting.add_circles(frame, text_lines, counter.class_labels, counter.class_colors)\n",
    "                frame = plotting.add_logo(frame)\n",
    "\n",
    "                writer.append_data(frame)\n",
    "                pbar.update(1)\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "        # Save tracking data to CSV\n",
    "        tracking_df = pd.DataFrame(tracking_data, columns=[\"frame_id\", \"track_id\", \"x\", \"y\", \"w\", \"h\", \"class_id\"])\n",
    "        tracking_df.to_csv(tracker_csv_output_path, index=False)\n",
    "        print(f\"Tracked video saved to {video_output_path}\")\n",
    "        print(f\"Tracker output saved to {tracker_csv_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a17012-b69c-4553-a81d-cf5edf66d20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate Processor\n",
    "processor = Processor()\n",
    "\n",
    "# Define paths\n",
    "video_path = '/home/jupyter/videos/GX010067.MP4'\n",
    "config_path = '/home/jupyter/weights/tomato/ddod_r152_Solanumlycopersicum13.py'\n",
    "checkpoint_path = '/home/jupyter/weights/tomato/best_coco_bbox_mAP_50_epoch_1176.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac33b0-0081-4e70-9e78-8da47cde9c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run detector\n",
    "processor.run_detector(video_path, config_path, checkpoint_path, use_slicing=True, confidence_threshold=0.35, reduce_fps=None, reduce_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f200edc4-6572-4c95-a099-635a4e2e596a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run tracker directly\n",
    "processor.run_tracker(line_offset=0,\n",
    "                      draw_line=True,\n",
    "                      track_high_thresh=0.6,\n",
    "                      track_low_thresh=0.1,\n",
    "                      new_track_thresh=0.7,\n",
    "                      track_buffer=15,\n",
    "                      proximity_thresh=0.4,\n",
    "                      cmc_method='sparseOptFlow',\n",
    "                      match_thresh=0.7,\n",
    "                      use_tracker_only=False,\n",
    "                      detections_path=None,\n",
    "                      video_path=video_path,\n",
    "                      config_path=config_path,\n",
    "                      checkpoint_path=checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
